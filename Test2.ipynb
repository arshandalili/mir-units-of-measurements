{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /Users/arshandalili/opt/anaconda3/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in /Users/arshandalili/opt/anaconda3/lib/python3.9/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /Users/arshandalili/opt/anaconda3/lib/python3.9/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: six in /Users/arshandalili/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from number_extractor import *\n",
    "from unit_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('یک', 'NUM'),\n",
       " ('خودرو', 'N'),\n",
       " ('با', 'P'),\n",
       " ('طول', 'Ne'),\n",
       " ('دراز', 'AJ'),\n",
       " ('و', 'CONJ'),\n",
       " ('عرض', 'Ne'),\n",
       " ('کوتاه', 'AJ'),\n",
       " ('از', 'P'),\n",
       " ('ما', 'PRO'),\n",
       " ('سبقت', 'N'),\n",
       " ('گرفت', 'V'),\n",
       " ('.', 'PUNC')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "\n",
    "text = 'یک خودرو با طول دراز و عرض کوتاه از ما سبقت گرفت.'\n",
    "\n",
    "tagger = POSTagger(model='resources/hazm/postagger.model')\n",
    "tagger.tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ماشین', 'Ne'), ('بسیار', 'ADV'), ('فوق\\u200cالعاده', 'ADV'), ('زیبا', 'AJ')]\n",
      "[('غذای', 'Ne'), ('خوب', 'AJe')]\n",
      "[('شکم', 'Ne'), ('چاق', 'AJ')]\n"
     ]
    }
   ],
   "source": [
    "def tree_generator(sentence):\n",
    "    grammar = r\"\"\"\n",
    "        NADJ: {<N|Ne><ADV>*<AJ|AJe>}\n",
    "    \"\"\"\n",
    "    return nltk.RegexpParser(grammar).parse(sentence)\n",
    "\n",
    "noun_and_adj_list = []\n",
    "for sentence in sent_tokenize('ماشین بسیار فوق‌العاده زیبا غذای خوب مداد علی شکم چاق'):\n",
    "    tags = tagger.tag(word_tokenize(sentence))\n",
    "    tree = tree_generator(tags)\n",
    "    for subtree in tree.subtrees():\n",
    "        # print(subtree.leaves())\n",
    "        if subtree.label() == 'NADJ':\n",
    "            noun_and_adj_list.append(subtree.leaves())\n",
    "            print(subtree.leaves())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'جابهجایی'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Normalizer()\n",
    "p.normalize('جابهجایی')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = hazm.Chunker(model='resources/hazm/chunker.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'توان مفید باتری گوشی علی صد وات است.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[توان مفید باتری گوشی علی صد وات NP] [است VP] .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = tagger.tag(hazm.word_tokenize(text))\n",
    "a = chunker.parse(tagged)\n",
    "b = hazm.tree2brackets(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wn/34vrybx169vc8t6g7nkw8gk00000gn/T/ipykernel_44740/2739686821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "c = a[0]\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ' '.join([s[0] for s in list(a[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'سرعت بالای خودروی عمه علی'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('سرعت', 'Ne'), ('بالای', 'AJe'), ('خودروی', 'Ne'), ('عمه', 'N'), ('علی', 'N')]),\n",
       " Tree('ADJP', [('عجیب', 'AJ')]),\n",
       " Tree('VP', [('است', 'V')]),\n",
       " ('.', 'PUNC')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import hazm\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "\n",
    "tagger = hazm.POSTagger(model='resources/hazm/postagger.model')\n",
    "chunker = hazm.Chunker(model='resources/hazm/chunker.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_alternative = {}\n",
    "with open('resources/dataset/quantities_word_network.json', 'r', encoding='utf-8') as fp:\n",
    "    inverse_alternative = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_generator(sentence):\n",
    "    grammar = r\"\"\"\n",
    "        NADJ: {<N|Ne><ADV>*<AJ|AJe|ADV>}\n",
    "    \"\"\"\n",
    "    return nltk.RegexpParser(grammar).parse(sentence)\n",
    "\n",
    "def extract_qualitative_expressions(text, tagger, inverse_alternative):\n",
    "    results = []\n",
    "    for sentence in hazm.sent_tokenize(text):\n",
    "        tags = tagger.tag(hazm.word_tokenize(sentence))\n",
    "        tree = tree_generator(tags)\n",
    "        last_index = 0\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NADJ':\n",
    "                try:\n",
    "                    in_context_POS = subtree.leaves()[-1][1]\n",
    "                    out_of_context_POS = tagger.tag([subtree.leaves()[-1][0]])[0][1]\n",
    "                    if in_context_POS.startswith(\"ADV\") and out_of_context_POS.startswith(\"ADV\"):\n",
    "                        continue\n",
    "                    try:\n",
    "                        res_type = inverse_alternative[subtree.leaves()[0][0]]\n",
    "                    except:\n",
    "                        y_count = -1\n",
    "                        while subtree.leaves()[0][0][y_count] == 'ی':\n",
    "                            y_count -= 1\n",
    "                        if y_count == -1:\n",
    "                            continue\n",
    "                        res_type = inverse_alternative[subtree.leaves()[0][0][:y_count + 1]]\n",
    "                    marker = ' '.join([s[0] for s in subtree.leaves()])\n",
    "                    span = re.search(marker, text[last_index:]).span()\n",
    "                    span = [span[0] + last_index, span[1] + last_index]\n",
    "                    last_index = span[1]\n",
    "                    res_dict = {\n",
    "                        'type': res_type,\n",
    "                        'amount': '',\n",
    "                        'unit': '',\n",
    "                        'item': '',\n",
    "                        'marker': marker,\n",
    "                        'span': span,\n",
    "                        'SI_equivalent': '',\n",
    "                    }\n",
    "                    results.append(res_dict)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'سرعت',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'سرعت زیاد',\n",
       "  'span': [24, 33],\n",
       "  'SI_equivalent': ''},\n",
       " {'type': 'دما',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'دمای بسیار سرد',\n",
       "  'span': [46, 60],\n",
       "  'SI_equivalent': ''},\n",
       " {'type': 'طول',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'ارتفاع حقیقتا بسیار بلند',\n",
       "  'span': [73, 97],\n",
       "  'SI_equivalent': ''},\n",
       " {'type': 'طول',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'قد کوتاه',\n",
       "  'span': [130, 138],\n",
       "  'SI_equivalent': ''},\n",
       " {'type': 'طول',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'قد نسبتا کوتاه',\n",
       "  'span': [242, 256],\n",
       "  'SI_equivalent': ''},\n",
       " {'type': 'طول',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'پهنایی به\\u200cغایت عریض',\n",
       "  'span': [259, 278],\n",
       "  'SI_equivalent': ''}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ما در ابتدا یک ماشین با سرعت زیاد دیدیم که در دمای بسیار سرد قطب جنوب در ارتفاع حقیقتا بسیار بلند در حال رانندگی بود. سپس مردی با قد کوتاه و شلوار خیلی بلند شروع به پیاده‌شدن کرد. قد آن مرد تقریباً برابر عرض حدودا متوسط آن ماشین بود. مردی با قد نسبتا کوتاه و پهنایی به‌غایت عریض'\n",
    "\n",
    "extract_qualitative_expressions(text, tagger, inverse_alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'طول'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_alternative['پهنا']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'شکم'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res_dict = {\n",
    "#     'type': inverse_alternative[NONE],\n",
    "#     'amount': '',\n",
    "#     'unit': '',\n",
    "#     'item': '',\n",
    "#     'marker': '',\n",
    "#     'span': '',\n",
    "# }\n",
    "\n",
    "subtree.leaves()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_patterns(patterns, grouping=False):\n",
    "    return '(' + ('?:' if not grouping else '') + '|'.join(patterns) + ')'\n",
    "\n",
    "\n",
    "def extract_qualitative(text, quantity, last_span):\n",
    "    tagged = tagger.tag(word_tokenize(text))\n",
    "    res = list(chunker.parse(tagged))\n",
    "    for tree in res:\n",
    "        tokens = list(tree)\n",
    "        for idx in range(len(tokens)):\n",
    "            if tokens[idx][0] == quantity and idx != len(tokens) - 1:\n",
    "                chunk_str = ' '.join([s[0] for s in tokens])\n",
    "                match_span = re.search(chunk_str, text).span()\n",
    "                adj_count = 0\n",
    "                while idx + adj_count + 1 < len(tokens) and tokens[idx + adj_count + 1][1].startswith('ADV'):\n",
    "                    adj_count += 1\n",
    "                target_token_pos = tokens[idx + adj_count + 1][1]\n",
    "                if (target_token_pos.startswith('AJ') or target_token_pos.startswith('ADJ')) and match_span[0] > last_span:\n",
    "                    return chunk_str\n",
    "\n",
    "def qualitive_descriptions(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    PATTERN = join_patterns(list(inverse_alternative.keys()))\n",
    "    res = []\n",
    "    read_chars = 0\n",
    "    for sent in sents:\n",
    "        last_span = -1\n",
    "        for match in re.finditer(PATTERN, sent):\n",
    "            marker = extract_qualitative(sent, match.group(), last_span)\n",
    "            if marker is None:\n",
    "                continue\n",
    "            temp_dict = {\n",
    "                'type': inverse_alternative[match.group()],\n",
    "                'amount': '',\n",
    "                'unit': '',\n",
    "                'item': '',\n",
    "                'marker': marker,\n",
    "                'span': [match.span()[0] + read_chars, match.span()[0] + len(marker) + read_chars]\n",
    "            }\n",
    "            last_span = match.span()[0] + len(marker) - 1\n",
    "            res.append(temp_dict)\n",
    "        read_chars += len(sent)\n",
    "        \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'طول',\n",
       "  'amount': '',\n",
       "  'unit': '',\n",
       "  'item': '',\n",
       "  'marker': 'طول بسیار زیبا و بسیار دراز',\n",
       "  'span': [0, 27]}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'یک خودرو با طول دراز و عرض کوتاه از ما سبقت گرفت.'\n",
    "text = 'طول بسیار زیبا و بسیار دراز خودروی علی شگفت‌انگیز است.'\n",
    "\n",
    "b = 1 * text\n",
    "\n",
    "b = p.normalize(b)\n",
    "\n",
    "qualitive_descriptions(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "quantities = {}\n",
    "with open('resources/dataset/quantities_related_names.json', 'r', encoding='utf-8') as fp:\n",
    "    quantities = json.load(fp)\n",
    "\n",
    "quantities['طول'].append('پهنا')\n",
    "\n",
    "temp = {}\n",
    "\n",
    "for k, v in quantities.items():\n",
    "    for alt in v:\n",
    "        if alt not in temp.keys():\n",
    "            temp[alt] = k\n",
    "\n",
    "with open('resources/dataset/inverse_alternative.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(temp, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('سرعت', 'Ne')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "quantities = {}\n",
    "with open('resources/dataset/quantities_related_names.json', 'r', encoding='utf-8') as fp:\n",
    "    quantities = json.load(fp)\n",
    "\n",
    "quantities_names = []\n",
    "for k, v in quantities.items():\n",
    "    quantities_names.extend(v)\n",
    "\n",
    "quantities_names\n",
    "\n",
    "with open('resources/dataset/alternatives_for_quantities.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(quantities_names, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تغییرات سرعت به تغییرات زمان',\n",
       " 'شتاب',\n",
       " 'تغییرات سرعت نسبت به تغییرات زمان',\n",
       " 'تغییرات سرعت نسبت به زمان',\n",
       " 'گشتاور دورانی',\n",
       " 'گشتاور زاویه\\u200cای',\n",
       " 'تکانه زاویه\\u200cای',\n",
       " 'سطح',\n",
       " 'زمین',\n",
       " 'مساحت',\n",
       " 'چگالی سطح',\n",
       " 'چگالی سطحی',\n",
       " 'جرم سطحی',\n",
       " 'ظرفیت',\n",
       " 'خازن',\n",
       " 'کاپاسیتانس',\n",
       " 'پتانسیل شیمیایی',\n",
       " 'آنتروپی',\n",
       " 'آنتالپی',\n",
       " 'چگالی جریان',\n",
       " 'جریان الکتریکی بر واحد سطح',\n",
       " 'بار الکتریکی',\n",
       " 'بار',\n",
       " 'الکتریسیته',\n",
       " 'پتانسیل الکتریکی',\n",
       " 'ولتاژ',\n",
       " 'پتانسل',\n",
       " 'اختلاف پتانسیل',\n",
       " 'مقاومت الکتریکی',\n",
       " 'مقاومت',\n",
       " 'رزیستانس',\n",
       " 'انرژی',\n",
       " 'گرما',\n",
       " 'ارزش غذایی',\n",
       " 'کارمایه',\n",
       " 'پتانسیل',\n",
       " 'جنبشی',\n",
       " 'آنتروپی',\n",
       " 'آنتالپی',\n",
       " 'توان',\n",
       " 'نرخ',\n",
       " 'فشار',\n",
       " 'گشتاور',\n",
       " 'نیروی چرخشی',\n",
       " 'نیروی دورانی',\n",
       " 'ممان نیرو',\n",
       " 'سرعت',\n",
       " 'تندی',\n",
       " 'جابجایی نسبت به زمان',\n",
       " 'مسافت نسبت به زمان',\n",
       " 'جابجایی نسبت به تغییرات زمان',\n",
       " 'مسافت نسبت به تغییرات زمان',\n",
       " 'مسافت طی شده نسبت به تغییرات زمان',\n",
       " 'حجم',\n",
       " 'فضا',\n",
       " 'طول',\n",
       " 'فاصله',\n",
       " 'عرض',\n",
       " 'ارتفاع',\n",
       " 'جابه\\u200cجایی',\n",
       " 'جا به جایی',\n",
       " 'جابجایی',\n",
       " 'مسافت',\n",
       " 'بازه',\n",
       " 'شعاع',\n",
       " 'قطر',\n",
       " 'ضلع',\n",
       " 'محیط',\n",
       " 'وتر',\n",
       " 'اضلاع',\n",
       " 'مکان',\n",
       " 'جرم',\n",
       " 'وزن',\n",
       " 'سنگینی',\n",
       " 'سبکی',\n",
       " 'زمان',\n",
       " 'مدت',\n",
       " 'بازه',\n",
       " 'طول',\n",
       " 'درازا',\n",
       " 'تایم',\n",
       " 'دما',\n",
       " 'گرمی',\n",
       " 'سردی',\n",
       " 'جوش',\n",
       " 'ذوب',\n",
       " 'تبخیر',\n",
       " 'میعان',\n",
       " 'انجماد',\n",
       " 'تصعید',\n",
       " 'تقطیر',\n",
       " 'چگالش',\n",
       " 'نقطه',\n",
       " 'مقدار ماده',\n",
       " 'تعداد ذرات',\n",
       " 'شدت روشنایی',\n",
       " 'نور',\n",
       " 'گسیل نوری',\n",
       " 'چگالی شار نوری',\n",
       " 'درخشش',\n",
       " 'درخشندگی',\n",
       " 'نیرو',\n",
       " 'قوه',\n",
       " 'زور',\n",
       " 'وزن',\n",
       " 'گرانش',\n",
       " 'جاذبه',\n",
       " 'دافعه',\n",
       " 'اصطکاک',\n",
       " 'گریز از مرکز',\n",
       " 'جانب مرکز',\n",
       " 'فرکانس',\n",
       " 'بسامد',\n",
       " 'ارتعاش',\n",
       " 'نوسان',\n",
       " 'جریان الکتریکی',\n",
       " 'جریان',\n",
       " 'آمپراژ',\n",
       " 'چگالی بار الکتریکی',\n",
       " 'چگالی',\n",
       " 'بار حجمی']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternatives = {}\n",
    "with open('resources/dataset/alternatives_for_quantities.json', 'r', encoding='utf-8') as fp:\n",
    "    alternatives = json.load(fp)\n",
    "\n",
    "alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extracted_quantity:\n",
    "\n",
    "    def __init(self, type, amount, unit, item, marker, span, SI_equivalent):\n",
    "        self.type = type\n",
    "        self.amount = amount\n",
    "        self.unit = unit\n",
    "        self.item = item\n",
    "        self.marker = marker\n",
    "        self.span = span\n",
    "        self.SI_equivalent = SI_equivalent\n",
    "\n",
    "    def get_dict(self):\n",
    "        return {\n",
    "            'type': self.type,\n",
    "            'amount': self.amount,\n",
    "            'unit': self.unit,\n",
    "            'item': self.item,\n",
    "            'marker': self.marker,\n",
    "            'span': self.span,\n",
    "            'SI_equivalent': self.SI_equivalent,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = hazm.Chunker(model='resources/hazm/chunker.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantities_word_network = {}\n",
    "with open('resources/dataset/quantities_word_network.json', 'r', encoding='utf-8') as file:\n",
    "    quantities_word_network = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(target):\n",
    "    return [key for key, value in quantities_word_network.items() if value == target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words_dict = {}\n",
    "with open('resources/dataset/quantities_related_names.json', 'r', encoding='utf-8') as file:\n",
    "    related_words_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'سرعت',\n",
       "  'amount': [15.0],\n",
       "  'unit': 'km/s',\n",
       "  'item': '',\n",
       "  'marker': 'تندی ۱۵ km/s',\n",
       "  'span': [13, 25],\n",
       "  'SI_equivalent': [15000.0]}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WS = WHITE_SPACE\n",
    "NUM_CONNECTOR = join_patterns([rf'({WS})?(?:،|,)({WS})?', rf'{WS}و{WS}'])\n",
    "NUM_SEQUENCE = rf'{NUM_IN_TEXT}(?:{NUM_CONNECTOR}{NUM_IN_TEXT})*'\n",
    "NUM_UNIT = rf'{NUM_SEQUENCE}{WHITE_SPACE}{COMPLEX_UNIT}'\n",
    "\n",
    "text = 'پهنای باتری موبایل پانزده میلی‌متر قرار بود باشد و هست'\n",
    "text = 'توان مفید باتری علی صد وات به نظر می‌رسد.'\n",
    "text = 'شهاب‌سنگی به تندی ۱۵ km/s وارد جو زمین شد.'\n",
    "# text = 'علی ۳.۵ کیلوگرم آرد خرید و باتری خود را هشتاد و پنج صدم وات شارژ کرد.'\n",
    "# text = 'علی ۳.۵ کیلوگرم جرم دارد.'\n",
    "# text = 'دیدیم وزن علی ۳.۵ کیلوگرم و جرم حسن ۵ کیلوگرم است.'\n",
    "# text = 'سلام! توان مفید باتری من صد وات به نظر می‌رسد.'\n",
    "# text = 'علی ۳.۵ کیلوگرم جرم دارد.'\n",
    "# text = 'علی ۵ کیلوگرم از آرد همسایه قرض گرفت.'\n",
    "\n",
    "\n",
    "def extract_expressions(text, tagger, chunker):\n",
    "\n",
    "    def get_nth_chunk(text, n, tagger, chunker):\n",
    "        tagged = tagger.tag(hazm.word_tokenize(text))\n",
    "        chunks = chunker.parse(tagged)\n",
    "        return ' '.join([c[0] for c in chunker.parse(tagged)[n]])\n",
    "    \n",
    "    result = []\n",
    "    for match in re.finditer(NUM_UNIT, text):\n",
    "        before = text[:match.span()[0] - 1]\n",
    "        after = text[match.span()[1]:]\n",
    "        before_chunk = get_nth_chunk(before, -1, tagger, chunker)\n",
    "        after_chunk = get_nth_chunk(after, 0, tagger, chunker)\n",
    "\n",
    "        unit = extract_units(match.group())[0]['object']\n",
    "        quantity = unit_to_quantity(unit)[0]\n",
    "        numbers = extract_numbers(match.group())\n",
    "        related_words = related_words_dict[quantity]\n",
    "        pattern = join_patterns(related_words)\n",
    "\n",
    "        before_matches = [matched for matched in re.finditer(pattern, before_chunk)]\n",
    "        before_match = None\n",
    "        span = list(match.span())\n",
    "        did_preextend = False\n",
    "        did_postextend = False\n",
    "        if before_matches:\n",
    "            before_match = before_matches[-1]\n",
    "            span[0] = match.span()[0] - len(before_chunk) + before_match.span()[0] - 1\n",
    "            did_preextend = True\n",
    "        if not did_preextend:\n",
    "            after_matches = [matched for matched in re.finditer(pattern, after_chunk)]\n",
    "            after_match = None\n",
    "            if after_matches:\n",
    "                after_match = after_matches[0]\n",
    "                span[1] = match.span()[1] + after_match.span()[1] + 1\n",
    "                did_postextend = True\n",
    "\n",
    "        item_word = ''\n",
    "        if not did_postextend:\n",
    "            tags = tagger.tag(hazm.word_tokenize(after_chunk))\n",
    "            if tags and tags[0][1].startswith('N'):\n",
    "                if len(tags) < 2 or not tags[1][1].startswith('V'):\n",
    "                    item_word = tags[0][0]\n",
    "                    item_match = re.search(item_word, after_chunk)\n",
    "                    span[1] = match.span()[1] + item_match.span()[1] + 1\n",
    "        \n",
    "        res_amount = [item['value'] for item in numbers]\n",
    "        SI_coeff = unit_to_SI_unit(unit).magnitude\n",
    "        SI_equivalent = [SI_coeff * item for item in res_amount]\n",
    "        marker = text[span[0]:span[1]]\n",
    "\n",
    "        unit_span = [numbers[0]['span'][0], numbers[-1]['span'][1]]\n",
    "        for number in numbers:\n",
    "            unit_span[0] = min(unit_span[0], number['span'][0])\n",
    "            unit_span[1] = max(unit_span[1], number['span'][1])\n",
    "\n",
    "        unit_str = match.group()[:unit_span[0]] + match.group()[unit_span[1]:].strip()\n",
    "\n",
    "        res_dict = {\n",
    "            'type': quantity,\n",
    "            'amount': res_amount,\n",
    "            'unit': unit_str,\n",
    "            'item': item_word,\n",
    "            'marker': marker,\n",
    "            'span': span,\n",
    "            'SI_equivalent': SI_equivalent,\n",
    "        }\n",
    "        result.append(res_dict)\n",
    "    return result\n",
    "\n",
    "extract_expressions(text, tagger, chunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a78e5bf1b2671d60bd6da117be21fdbc9fac13762871a45216d3927d388d0255"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
